name: Development Workflow

on:
  push:
    branches: [ develop, feature/*, bugfix/* ]
  pull_request:
    branches: [ develop ]

jobs:
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install uv
      uses: astral-sh/setup-uv@v2
      with:
        version: "latest"
    
    - name: Install dependencies
      run: |
        uv venv
        uv pip install -e ".[dev]"
    
    - name: Check code formatting
      run: |
        uv run black --check src tests scripts
        uv run isort --check-only src tests scripts
    
    - name: Run linting
      run: |
        uv run flake8 src tests scripts
        uv run mypy src
    
    - name: Security audit
      run: |
        uv pip install pip-audit
        uv run pip-audit

  test-matrix:
    name: Test Matrix
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.8", "3.9", "3.10", "3.11", "3.12"]
        exclude:
          # Reduce matrix size for development workflow
          - os: windows-latest
            python-version: "3.8"
          - os: windows-latest
            python-version: "3.9"
          - os: macos-latest
            python-version: "3.8"
          - os: macos-latest
            python-version: "3.9"
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install uv
      uses: astral-sh/setup-uv@v2
      with:
        version: "latest"
    
    - name: Install dependencies
      run: |
        uv venv
        uv pip install -e ".[dev]"
    
    - name: Run unit tests
      run: |
        uv run pytest tests/ -m "unit" --cov-report=xml --cov-report=term-missing
    
    - name: Run fast integration tests
      run: |
        uv run pytest tests/ -m "integration and not slow" -v
    
    - name: Upload coverage (Ubuntu Python 3.11 only)
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  development-tests:
    name: Development Testing
    runs-on: ubuntu-latest
    needs: [code-quality]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install uv
      uses: astral-sh/setup-uv@v2
      with:
        version: "latest"
    
    - name: Install dependencies
      run: |
        uv venv
        uv pip install -e ".[dev]"
    
    - name: Run development test script
      run: |
        python scripts/dev_test.py --workflow ci --skip-prereq
    
    - name: Run integration test suite
      run: |
        python run_integration_tests.py --include-slow
    
    - name: Upload test artifacts
      if: failure()
      uses: actions/upload-artifact@v3
      with:
        name: test-artifacts
        path: |
          htmlcov/
          .coverage
          pytest-report.xml
        retention-days: 7

  package-validation:
    name: Package Validation
    runs-on: ubuntu-latest
    needs: [test-matrix]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build twine uv
    
    - name: Build package
      run: python -m build
    
    - name: Check package
      run: twine check dist/*
    
    - name: Validate distribution
      run: python scripts/validate_distribution.py --skip-uvx
    
    - name: Test wheel installation
      run: |
        python -m venv test-env
        source test-env/bin/activate
        pip install dist/*.whl
        databricks-mcp-server --help
    
    - name: Upload package artifacts
      uses: actions/upload-artifact@v3
      with:
        name: package-dist
        path: dist/
        retention-days: 7

  cross-platform-validation:
    name: Cross-Platform Package Test
    runs-on: ${{ matrix.os }}
    needs: [package-validation]
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install uv
      uses: astral-sh/setup-uv@v2
      with:
        version: "latest"
    
    - name: Download package artifacts
      uses: actions/download-artifact@v3
      with:
        name: package-dist
        path: dist/
    
    - name: Test package installation
      shell: bash
      run: |
        # Test pip installation
        python -m venv test-pip
        if [[ "$RUNNER_OS" == "Windows" ]]; then
          source test-pip/Scripts/activate
        else
          source test-pip/bin/activate
        fi
        pip install dist/*.whl
        databricks-mcp-server --help
        deactivate
    
    - name: Test uvx installation
      shell: bash
      run: |
        # Test uvx installation
        uvx --from dist/*.whl databricks-mcp-server --help
    
    - name: Run cross-platform tests
      run: |
        uv venv
        uv pip install -e ".[dev]"
        python test_cross_platform.py --skip-uvx

  documentation-check:
    name: Documentation Check
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Check documentation completeness
      run: |
        # Check that all required documentation files exist
        files=(
          "README.md"
          "DEVELOPMENT.md"
          "CONTRIBUTING.md"
          "INTEGRATION_TESTING.md"
          "BUILD_AND_DISTRIBUTION.md"
          "TROUBLESHOOTING.md"
          "ENVIRONMENT_VARIABLES.md"
          "config/config.yaml.example"
        )
        
        for file in "${files[@]}"; do
          if [[ ! -f "$file" ]]; then
            echo "Missing documentation file: $file"
            exit 1
          fi
        done
        
        echo "All required documentation files present"
    
    - name: Validate configuration example
      run: |
        python -c "
        import yaml
        with open('config/config.yaml.example') as f:
            config = yaml.safe_load(f)
        print('Configuration example is valid YAML')
        "
    
    - name: Check README links
      run: |
        # Basic check for common broken link patterns
        if grep -q "](http" README.md; then
          echo "Found external links in README"
        fi
        
        # Check for placeholder text
        if grep -qi "TODO\|FIXME\|XXX" README.md; then
          echo "Found TODO/FIXME in README"
          exit 1
        fi

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit semgrep
    
    - name: Run safety check
      run: |
        pip freeze | safety check --stdin
    
    - name: Run bandit security scan
      run: |
        bandit -r src/ -f json -o bandit-report.json || true
        bandit -r src/
    
    - name: Upload security scan results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: security-scan-results
        path: |
          bandit-report.json
        retention-days: 30

  performance-check:
    name: Performance Check
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Install uv
      uses: astral-sh/setup-uv@v2
      with:
        version: "latest"
    
    - name: Install dependencies
      run: |
        uv venv
        uv pip install -e ".[dev]"
        uv pip install memory-profiler psutil
    
    - name: Test startup performance
      run: |
        python -c "
        import time
        import subprocess
        
        # Test server startup time
        start = time.time()
        result = subprocess.run(['databricks-mcp-server', '--help'], 
                              capture_output=True, timeout=30)
        elapsed = time.time() - start
        
        print(f'Startup time: {elapsed:.2f}s')
        if elapsed > 5.0:
            print('WARNING: Startup time is slow')
            exit(1)
        
        print('Startup performance acceptable')
        "
    
    - name: Memory usage check
      run: |
        python -c "
        import subprocess
        import psutil
        import time
        
        # Start server process
        proc = subprocess.Popen(['databricks-mcp-server', '--help'], 
                               stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        
        # Wait a moment for process to start
        time.sleep(1)
        
        try:
            process = psutil.Process(proc.pid)
            memory_mb = process.memory_info().rss / 1024 / 1024
            print(f'Memory usage: {memory_mb:.1f} MB')
            
            if memory_mb > 100:
                print('WARNING: High memory usage')
            else:
                print('Memory usage acceptable')
        except psutil.NoSuchProcess:
            print('Process completed quickly')
        finally:
            proc.terminate()
            proc.wait()
        "

  summary:
    name: Development Workflow Summary
    runs-on: ubuntu-latest
    needs: [
      code-quality,
      test-matrix,
      development-tests,
      package-validation,
      cross-platform-validation,
      documentation-check,
      security-scan,
      performance-check
    ]
    if: always()
    
    steps:
    - name: Check workflow results
      run: |
        echo "Development Workflow Results:"
        echo "- Code Quality: ${{ needs.code-quality.result }}"
        echo "- Test Matrix: ${{ needs.test-matrix.result }}"
        echo "- Development Tests: ${{ needs.development-tests.result }}"
        echo "- Package Validation: ${{ needs.package-validation.result }}"
        echo "- Cross-Platform: ${{ needs.cross-platform-validation.result }}"
        echo "- Documentation: ${{ needs.documentation-check.result }}"
        echo "- Security Scan: ${{ needs.security-scan.result }}"
        echo "- Performance: ${{ needs.performance-check.result }}"
        
        # Check if any critical jobs failed
        if [[ "${{ needs.code-quality.result }}" == "failure" ]] || \
           [[ "${{ needs.test-matrix.result }}" == "failure" ]] || \
           [[ "${{ needs.development-tests.result }}" == "failure" ]]; then
          echo "❌ Critical development checks failed"
          exit 1
        else
          echo "✅ Development workflow completed successfully"
        fi